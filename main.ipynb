{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea1caa-de33-405b-8819-1ba0a4e208a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from email.utils import parsedate_to_datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg\n",
    "from sqlalchemy import create_engine, text\n",
    "import shutil\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "#####################################################\n",
    "# NORTH CAROLINA STATE BOARD OF ELECTIONS PUBLIC DATA\n",
    "#####################################################\n",
    "\n",
    "# URLs for the voter registration and history files\n",
    "VREG_URL = \"https://s3.amazonaws.com/dl.ncsbe.gov/data/ncvoter_Statewide.zip\"\n",
    "VHIS_URL = \"https://s3.amazonaws.com/dl.ncsbe.gov/data/ncvhis_Statewide.zip\"\n",
    "fileUrls = [VREG_URL, VHIS_URL]\n",
    "\n",
    "# Specifiy the encoding of the files\n",
    "FILE_ENCODING = \"latin-1\"\n",
    "\n",
    "# Get the download filenames, which are zip files\n",
    "vRegZipName = VREG_URL.split(\"/\")[-1]\n",
    "vHisZipName = VHIS_URL.split(\"/\")[-1]\n",
    "fileZips = [vRegZipName, vHisZipName]\n",
    "\n",
    "# Get the stems of the filenames\n",
    "vRegFileStem = vRegZipName.split(\".\")[0]\n",
    "vHisFileStem = vHisZipName.split(\".\")[0]\n",
    "vFileStems = [vRegFileStem]\n",
    "\n",
    "# print(f\"vRegZipName: {vRegZipName}\")\n",
    "# print(f\"vHisZipName: {vHisZipName}\")\n",
    "# print(f\"vRegFileStem: {vRegFileStem}\")\n",
    "# print(f\"vHisFileStem: {vHisFileStem}\")\n",
    "\n",
    "##################################\n",
    "# POSTGRESQL DB CONNECTION DETAILS\n",
    "##################################\n",
    "\n",
    "# Your PostgreSQL username\n",
    "DB_USER = \"\"\n",
    "# Tour PostgreSQL password\n",
    "DB_PASSWORD = \"\"\n",
    "# Your actual host\n",
    "DB_HOST = \"\"\n",
    "# Default PostgreSQL port\n",
    "DB_PORT = \"\"\n",
    "# Your target database name\n",
    "DB_NAME = \"\"\n",
    "# Your target database schema\n",
    "DB_SCHEMA = \"\"\n",
    "# Naming schema for your target table(s)\n",
    "TABLE_PREFIX = \"nc_\"\n",
    "TABLE_SUFFIX = \"_history\"\n",
    "\n",
    "dbPath = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "vRegTablename = TABLE_PREFIX + \"vreg\" + TABLE_SUFFIX\n",
    "vHisTablename = TABLE_PREFIX + \"vhis\" + TABLE_SUFFIX\n",
    "vRegTablePath = dbPath + \"/\" + vRegTablename\n",
    "vHisTablePath = dbPath + \"/\" + vHisTablename\n",
    "dbTablePaths = [vRegTablePath, vHisTablePath]\n",
    "\n",
    "# print(f\"dbPath: {dbPath}\")\n",
    "# print(f\"vRegTablename: {vRegTablename}\")\n",
    "# print(f\"vHisTablename: {vHisTablename}\")\n",
    "# print(f\"vRegTablePath: {vRegTablePath}\")\n",
    "# print(f\"vHisTablePath: {vHisTablePath}\")\n",
    "# print(f\"dbTablePaths: {dbTablePaths}\")\n",
    "\n",
    "##########################\n",
    "# FILE TO SQL RELATIONSHIP\n",
    "##########################\n",
    "\n",
    "voterFilesDict = {vRegFileStem: vRegTablePath, vHisFileStem: vHisTablePath}\n",
    "\n",
    "# print(voterFilesDict)\n",
    "\n",
    "################\n",
    "# DIRECTORY INFO\n",
    "################\n",
    "\n",
    "# Download folder path\n",
    "DOWNLOAD_PATH = r\"\"\n",
    "# Archive path\n",
    "ARCHIVE_PATH = r\"\"\n",
    "\n",
    "# print(f\"DOWNLOAD_PATH: {DOWNLOAD_PATH}\")\n",
    "# print(f\"ARCHIVE_PATH: {ARCHIVE_PATH}\")\n",
    "\n",
    "############\n",
    "# CORE LOGIC\n",
    "############\n",
    "\n",
    "def main():\n",
    "    # Conditional Bypass!\n",
    "    # If you're updating the voter history/registration files for the week and have already run the script but had errors after:\n",
    "    # a) successfully running the copySqlTablesToArchive() function the full way through\n",
    "    # b) sucessfully running the downloadFiles() function the full way through\n",
    "    # You can skip those functions by assigning the fileDate variable with the data_date for the current run\n",
    "    # The date should be formatted as a string and follow the pattern YYYYMMDD; for example, \"20250101\" or \"20241031\"\n",
    "    # If the fileDate is equal to \"YYYYMMDD\" then the code will run all the functions\n",
    "    # Otherwise, it will pick up with the processFiles() function\n",
    "    fileDate = \"YYYYMMDD\"\n",
    "    if fileDate == \"YYYYMMDD\":\n",
    "        # Run the full pipeline; archive >>> download >>> process\n",
    "        copySqlTablesToArchive(dbPath, dbTablePaths, ARCHIVE_PATH)\n",
    "        fileDate = downloadFiles(fileUrls, DOWNLOAD_PATH, ARCHIVE_PATH)\n",
    "        processFiles(DOWNLOAD_PATH, voterFilesDict, FILE_ENCODING, fileDate)\n",
    "        print(\"Processing complete!\")\n",
    "    else:\n",
    "        try:\n",
    "            # Validate the fileDate is in YYYYMMDD format\n",
    "            datetime.strptime(fileDate, \"%Y%m%d\")\n",
    "            # If valid, continue straight to the processFiles() function\n",
    "            processFiles(DOWNLOAD_PATH, voterFilesDict, FILE_ENCODING, fileDate)\n",
    "            print(\"Processing complete!\")\n",
    "        except ValueError:\n",
    "            print(f\"Invalid fileDate: {fileDate}. Must be a string in YYYYMMDD format.\")\n",
    "\n",
    "# At the start of the process, copy the target tables so that in the event something goes wrong while the script is running, the data is preserved\n",
    "# At the moment, the function requires three arguments:\n",
    "# The first is a string of the path to the database as a URL\n",
    "# The second is a list of the tables in the database that are going to be copied/archived; these should also be in URL format\n",
    "# The third is a path to the directory where the tables are going to be copied/arvhived to\n",
    "def copySqlTablesToArchive(databasePath, tablePathsList, destination):\n",
    "    for tablePath in tablePathsList:\n",
    "        # Get tablename\n",
    "        table = tablePath.split(\"/\")[-1]\n",
    "        # Define outfile name\n",
    "        # outfileName = f\"{destination}\\{table}.tsv\"\n",
    "        outfileName = os.path.join(destination, f\"{table}.tsv\")\n",
    "        # Connect to database\n",
    "        with psycopg.connect(databasePath) as conn:\n",
    "            # Open a cursor to perform database operations\n",
    "            with conn.cursor() as cur, open(outfileName, \"wb\") as outfile:\n",
    "                print(f\"Preparing to copy and archive the {table} table...\")\n",
    "                # Execute a command: this copies and then exports a table\n",
    "                cur.execute(\"START TRANSACTION ISOLATION LEVEL REPEATABLE READ READ ONLY;\")\n",
    "                copySql = f\"\"\"\n",
    "                COPY \"{table}\"\n",
    "                TO STDOUT WITH (FORMAT csv, DELIMITER E'\\\\t', HEADER true, ENCODING 'UTF8')\n",
    "                \"\"\"\n",
    "                with cur.copy(copySql) as cp:\n",
    "                    for chunk in cp:\n",
    "                        outfile.write(chunk)\n",
    "        print(\"Wrote: \", outfileName)\n",
    "\n",
    "# Download the zip files from the NC State Board of Elections\n",
    "# Requires a list of URLs to the zip files\n",
    "def downloadFiles(urlsList, downloadDir, archiveDir):\n",
    "    # Get the string that will populate the \"data_date\" field\n",
    "    # Both files from the state are refreshed on the same day; only one needs to be referenced for the correct date\n",
    "    dateStr = getLastModifiedDate(urlsList[0])\n",
    "    for url in urlsList:\n",
    "        try:\n",
    "            # Download the zip file\n",
    "            zipFilename = os.path.basename(url)\n",
    "            zipDest = os.path.join(downloadDir, zipFilename)\n",
    "            print(f\"Downloading {zipFilename} to {downloadDir}...\")\n",
    "            urllib.request.urlretrieve(url, zipDest)\n",
    "            # Create a copy of the download in the archive\n",
    "            copyDownloadToArchive(zipDest, archiveDir, dateStr)\n",
    "            # Unzip the download\n",
    "            unzipFile(zipDest)\n",
    "            # Delete the zip from the downloads; archive already has a copy\n",
    "            os.remove(zipDest)\n",
    "        except Exception:\n",
    "            print(f\"Error downloading:\\n{url}\")\n",
    "            print(f\"Exception:\\n{Exception}\")\n",
    "    return dateStr\n",
    "\n",
    "# Process the unzipped files\n",
    "# For the voter history file, this entails using an anti-join against the master table to identify what is new as the data in the public file is only looking back 10 years\n",
    "# For the voter registration file, this entails going through each record in the latest voter registration file to see if anything doesn't match exactly to the most recent data for each NCID\n",
    "# These changes can be anything, from a party affiliation change to moving to redistricting to becoming a year older\n",
    "# In fact, at the start of the year, every record will be treated as different because there is a field that contains the voters age at the end of the year\n",
    "def processFiles(downloadPath, filesDict, encoding, dateStr):\n",
    "    # Store the filenames for each file type; they will be processed in different ways\n",
    "    registrationFile = \"ncvoter_Statewide\"\n",
    "    historyFile = \"ncvhis_Statewide\"\n",
    "    for stem, sqlTable in filesDict.items():\n",
    "        # Construction the location of the file so it can be read into a dataframe\n",
    "        # fileLocation = f\"{downloadPath}\\{stem}.txt\"\n",
    "        fileLocation = os.path.join(downloadPath, f\"{stem}.txt\")\n",
    "        # Define the encoding; historically this has been latin-1 but it's possible this can change in the future\n",
    "        newDataDf = pd.read_csv(fileLocation, sep=\"\\t\", dtype=\"str\", na_filter=False, encoding=encoding)\n",
    "        print(f\"Successfully loaded {stem} into a dataframe.\")\n",
    "        # Delete the .txt file after successfully loading into dataframe\n",
    "        os.remove(fileLocation)\n",
    "        # Clean up the extraneous whitespace\n",
    "        newDataDf = normalizeWhitespace(newDataDf)\n",
    "        # Drop the table name from the table path in sqlTable\n",
    "        sqlDb = sqlTable.rsplit(\"/\", 1)[0]\n",
    "        if stem == registrationFile:\n",
    "            # Add the 'data_date' field\n",
    "            newDataDf[\"data_date\"] = dateStr\n",
    "            # Connect to the right table in the database\n",
    "            engine = create_engine(sqlDb, pool_pre_ping=True)\n",
    "            # Query the database; this query will return the latest record, using the data_date field, per NCID\n",
    "            # This will be used to determine if there have been any changes in the the latest data\n",
    "            query = text(f\"\"\"\n",
    "            SELECT DISTINCT ON (ncid) *\n",
    "            FROM {DB_SCHEMA}.{vRegTablename}\n",
    "            ORDER BY ncid, data_date DESC\n",
    "            \"\"\")\n",
    "            with engine.connect() as connection:\n",
    "                masterDataDf = pd.read_sql_query(query, connection)\n",
    "            # print(masterData.head(10))\n",
    "            # Ensure the same column order as in the SQL table\n",
    "            newDataDf = newDataDf[masterDataDf.columns]\n",
    "            # Append the new data to the master data (the most recent data from the SQL database)\n",
    "            combinedDataDf = pd.concat([masterDataDf, newDataDf], ignore_index=True)\n",
    "            # I am only interested in records from the new data that are actually new\n",
    "            # This will be determined by ignoring the data_date field and searching for duplication\n",
    "            # First, create a list of the field names except data_date; this will be used as a subset in a drop_duplicates() function call\n",
    "            fieldSubset = list(combinedDataDf.columns.drop(\"data_date\"))\n",
    "            # Then deduplicate the data\n",
    "            dedupedDataDf = combinedDataDf.drop_duplicates(subset=fieldSubset, keep=\"first\")\n",
    "            # Filter the table to those where the data_date is from the latest batch\n",
    "            dedupedDataDf = dedupedDataDf[dedupedDataDf[\"data_date\"] == dateStr].reset_index(drop=True)\n",
    "            # Append the truly new data to the master table in SQL\n",
    "            with engine.begin() as connection:\n",
    "                dedupedDataDf.to_sql(\n",
    "                    name=vRegTablename,\n",
    "                    schema=DB_SCHEMA,\n",
    "                    con=connection,\n",
    "                    if_exists=\"append\",\n",
    "                    index=False\n",
    "                )\n",
    "            print(f\"New data from ncvoter_Statewide.txt appended to {sqlTable}.\")\n",
    "        elif stem == historyFile:\n",
    "            # Connect to the right table in the database\n",
    "            engine = create_engine(sqlDb, pool_pre_ping=True)\n",
    "            # Query the database; this query will return everything\n",
    "            # This will be used to determine if there have been any changes in the the latest data\n",
    "            query = text(f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {DB_SCHEMA}.{vHisTablename}\n",
    "            \"\"\")\n",
    "            with engine.connect() as connection:\n",
    "                masterDataDf = pd.read_sql_query(query, connection)\n",
    "            # print(masterData.head(10))\n",
    "            # Join the new data to the master data (the most recent data from the SQL database)\n",
    "            # Ensure the columns align exactly (order and names)\n",
    "            cols = list(masterDataDf.columns)\n",
    "            newDataDf = newDataDf[cols].copy()\n",
    "            mergedDf = newDataDf.merge(masterDataDf, how=\"left\", on=cols, indicator=True)\n",
    "            dedupedDataDf = mergedDf.loc[mergedDf[\"_merge\"] == \"left_only\", cols].reset_index(drop=True)\n",
    "            # Append the truly new data to the master table in SQL\n",
    "            if not dedupedDataDf.empty:\n",
    "                with engine.begin() as connection:\n",
    "                    dedupedDataDf.to_sql(\n",
    "                        name=vHisTablename,\n",
    "                        schema=DB_SCHEMA,\n",
    "                        con=connection,\n",
    "                        if_exists=\"append\",\n",
    "                        index=False\n",
    "                    )\n",
    "                print(f\"New data from ncvhis_Statewide.txt appended to {sqlTable}.\")\n",
    "\n",
    "# Obtain the \"Last Modified\" date on the new NC voter files\n",
    "# This is used to populate the 'data_date' field in the destination table(s)\n",
    "# The 'data_date' is the date that exact record was published\n",
    "# If \"Last Modified\" cannot be obtained, the current date will be used instead\n",
    "def getLastModifiedDate(downloadUrl):\n",
    "    # Try HEAD first\n",
    "    try:\n",
    "        req = urllib.request.Request(downloadUrl, method=\"HEAD\")\n",
    "        with urllib.request.urlopen(req) as resp:\n",
    "            lastModified = resp.headers.get(\"Last-Modified\")\n",
    "    # Fallback to GET\n",
    "    except Exception:\n",
    "        with urllib.request.urlopen(downloadUrl) as resp:\n",
    "            lastModified = resp.headers.get(\"Last-Modified\")\n",
    "    if lastModified:\n",
    "        try:\n",
    "            modificationDate = parsedate_to_datetime(lastModified)\n",
    "            modificationDate = modificationDate.strftime(\"%Y%m%d\")\n",
    "            return modificationDate\n",
    "        except:\n",
    "            # Proceed to use the current date\n",
    "            pass\n",
    "    else:\n",
    "        print(f\"Unable to access/determine the modification date of the file for {downloadUrl}. Using the current date instead.\")\n",
    "        currentDate = datetime.now(datetime.timezone.utc).date()\n",
    "        currentDate = currentDate.strftime(\"%Y%m%d\")\n",
    "        return currentDate\n",
    "\n",
    "# Normalize the whitespace in a dataframe\n",
    "def normalizeWhitespace(dataframe):\n",
    "    dataframe = dataframe.apply(lambda col: col.map(lambda x: ' '.join(x.split()) if isinstance(x, str) else x))\n",
    "    return dataframe\n",
    "\n",
    "# Copy the downloaded zip file to the archive to have a copy in case it's needed in the future\n",
    "# Call this before you unzip and process the data\n",
    "# Requires two argument:\n",
    "# 1- the downloaded file; a zip in this case\n",
    "# 2- the path to the directory where the downloaded will be preserved\n",
    "# 3- the date string that will become the prefix of the filename as it's being copied; will signify the date of the data/download\n",
    "def copyDownloadToArchive(download, archiveDir, prefixDate):\n",
    "    filename = os.path.basename(download)\n",
    "    archivedFilename = f\"{prefixDate}_{filename}\"\n",
    "    finalArchivePath = os.path.join(archiveDir, archivedFilename)\n",
    "    shutil.copy(download, finalArchivePath)\n",
    "    print(f\"A copy of {filename} has been copied to {archiveDir} as {archivedFilename}.\")\n",
    "\n",
    "# Unzips a zip file; the zip file is unzipped in the same location as the zip file\n",
    "def unzipFile(zipPath):\n",
    "    zipName = os.path.basename(zipPath)\n",
    "    zipDir = os.path.dirname(zipPath)\n",
    "    with zipfile.ZipFile(zipPath, mode='r') as zippedFile:\n",
    "        zippedFile.extractall(zipDir)\n",
    "    print(f\"{zipName} has been unzipped in {zipDir}\")\n",
    "\n",
    "########################\n",
    "# CALL THE MAIN FUNCTION\n",
    "########################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
